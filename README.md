# Local AI
Simple chat interface built in React and Vite to interact with AI models offline using Ollama.
- Select from locally downloaded models from Ollama
- Live response streaming
- Long term conversation memory saved to local storage
- Upload text, CSV, or JSON files, currently limited due to context size

## Resources
- [Ollama](https://ollama.com/)
- [llama3.1 Model](https://ollama.com/library/llama3.1)
- [mistral Model](https://ollama.com/library/mistral)
- [Ollama JavaScript Library](https://www.npmjs.com/package/ollama)

## TODO
- Switch between different chats
- Render Markdown cleanly
- Clean up uploaded file displaying plaintext during chat